{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code by Alexander Fred Ojala https://github.com/ikhlaqsidhu/data-x/blob/master/06b-tools-tensorflow/intro-to-tf_v2_afo.ipynb\n",
    "\n",
    "# TensorBoard Graph visualizer in notebook\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script src=\"//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js\"></script>\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "with open(\"ner/nyt_data_dict.json\") as f:\n",
    "    data_dict=json.load(f)\n",
    "    \n",
    "from ner.ner.corpus import Corpus\n",
    "\n",
    "corp = Corpus(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: \n",
      "Embeddings 6216912\n",
      "ConvNet 168192\n",
      "Classifier 1028\n",
      "transitions:0 16\n",
      "Total number of parameters equal 6386148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keeley\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# out-of-the-box model initialization\n",
    "from ner.ner.network import NER\n",
    "\n",
    "model_params = {\"use_batch_norm\": True,\n",
    "                \"use_crf\": True,\n",
    "                \"net_type\": 'cnn',\n",
    "                \"use_capitalization\": True,\n",
    "               }\n",
    "\n",
    "net = NER(corp, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 3112 phrases; correct: 2502.\n",
      "\n",
      "precision:  80.40%; recall:  83.62%; FB1:  81.98\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 2899 phrases; correct: 2445.\n",
      "\n",
      "precision:  84.34%; recall:  81.72%; FB1:  83.01\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 3140 phrases; correct: 2545.\n",
      "\n",
      "precision:  81.05%; recall:  85.06%; FB1:  83.01\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 2671 phrases; correct: 2335.\n",
      "\n",
      "precision:  87.42%; recall:  78.04%; FB1:  82.47\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 2989 phrases; correct: 2456.\n",
      "\n",
      "precision:  82.17%; recall:  82.09%; FB1:  82.13\n",
      "\n",
      "\n",
      "Eval on train:\n",
      "processed 779446 tokens with 14562 phrases; found: 14562 phrases; correct: 14314.\n",
      "\n",
      "precision:  98.30%; recall:  98.30%; FB1:  98.30\n",
      "\n",
      "\tName: precision:  98.30%; recall:  98.30%; F1:  98.30 14562\n",
      "\n",
      "\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 2989 phrases; correct: 2456.\n",
      "\n",
      "precision:  82.17%; recall:  82.09%; FB1:  82.13\n",
      "\n",
      "\tName: precision:  82.17%; recall:  82.09%; F1:  82.13 2989\n",
      "\n",
      "\n",
      "Eval on test:\n",
      "processed 167190 tokens with 3174 phrases; found: 3157 phrases; correct: 2607.\n",
      "\n",
      "precision:  82.58%; recall:  82.14%; FB1:  82.36\n",
      "\n",
      "\tName: precision:  82.58%; recall:  82.14%; F1:  82.36 3157\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# out-of-the-box model training and eval on train, validation data\n",
    "results = net.fit(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save('../nm/ner/cnn_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This',\n",
       "  'week,',\n",
       "  'DC',\n",
       "  'Comics',\n",
       "  'released',\n",
       "  'a',\n",
       "  'flock',\n",
       "  'of',\n",
       "  'doves',\n",
       "  '.']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['This week, DC Comics released a flock of doves .'.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'O', 'B-Name', 'I-Name', 'O', 'O', 'O', 'O', 'O', 'O']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on new sentence\n",
    "new_x, new_y = corp.tokens_batch_to_numpy_batch(['This week, DC Comics released a flock of doves .'.split()], \n",
    "                                [['O', 'O', 'B-Name', 'I-Name', 'O', 'O', 'O', 'O', 'O']])\n",
    "net.predict(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: \n",
      "Embeddings 6216912\n",
      "RNN_layer_0 315392\n",
      "RNN_layer_1 1050624\n",
      "Classifier 2052\n",
      "transitions:0 16\n",
      "Total number of parameters equal 7584996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keeley\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "net2_params = {\"use_crf\": True,\n",
    "                \"net_type\": 'rnn',\n",
    "                \"use_capitalization\": True,\n",
    "                  \"cell_type\": 'lstm'}\n",
    "net2=NER(corp, **net2_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 3071 phrases; correct: 2429.\n",
      "\n",
      "precision:  79.09%; recall:  81.18%; FB1:  80.13\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 3233 phrases; correct: 2548.\n",
      "\n",
      "precision:  78.81%; recall:  85.16%; FB1:  81.86\n",
      "\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ca75ec373034>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\nm\\ner\\ner\\network.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, batch_gen, batch_size, learning_rate, epochs, dropout_rate, learning_rate_decay)\u001b[0m\n\u001b[0;32m    242\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbouse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_conll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#RNN. Early data suggests this is way the fuck slower than the cnn\n",
    "# terminated after ~20 min\n",
    "results2 = net2.fit(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: \n",
      "Embeddings 6216912\n",
      "ConvNet 168192\n",
      "Classifier 1028\n",
      "transitions:0 16\n",
      "Total number of parameters equal 6386148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keeley\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "net3_params = {\"use_batch_norm\": True,\n",
    "                \"use_crf\": True,\n",
    "                \"net_type\": 'cnn_highway',\n",
    "                \"use_capitalization\": True,\n",
    "               }\n",
    "net3=NER(corp, **net3_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 2863 phrases; correct: 2359.\n",
      "\n",
      "precision:  82.40%; recall:  78.84%; FB1:  80.58\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 2423 phrases; correct: 2142.\n",
      "\n",
      "precision:  88.40%; recall:  71.59%; FB1:  79.11\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 3203 phrases; correct: 2601.\n",
      "\n",
      "precision:  81.21%; recall:  86.93%; FB1:  83.97\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 3289 phrases; correct: 2629.\n",
      "\n",
      "precision:  79.93%; recall:  87.87%; FB1:  83.71\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 3001 phrases; correct: 2515.\n",
      "\n",
      "precision:  83.81%; recall:  84.06%; FB1:  83.93\n",
      "\n",
      "\n",
      "Eval on train:\n",
      "processed 779446 tokens with 14562 phrases; found: 14736 phrases; correct: 14332.\n",
      "\n",
      "precision:  97.26%; recall:  98.42%; FB1:  97.84\n",
      "\n",
      "\tName: precision:  97.26%; recall:  98.42%; F1:  97.84 14736\n",
      "\n",
      "\n",
      "Eval on valid:\n",
      "processed 167043 tokens with 2992 phrases; found: 3001 phrases; correct: 2515.\n",
      "\n",
      "precision:  83.81%; recall:  84.06%; FB1:  83.93\n",
      "\n",
      "\tName: precision:  83.81%; recall:  84.06%; F1:  83.93 3001\n",
      "\n",
      "\n",
      "Eval on test:\n",
      "processed 167190 tokens with 3174 phrases; found: 3204 phrases; correct: 2682.\n",
      "\n",
      "precision:  83.71%; recall:  84.50%; FB1:  84.10\n",
      "\n",
      "\tName: precision:  83.71%; recall:  84.50%; F1:  84.10 3204\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Name',\n",
       "              OrderedDict([('precision', 83.70786516853933),\n",
       "                           ('recall', 84.49905482041588),\n",
       "                           ('f1', 84.10159924741298),\n",
       "                           ('n_predicted_entities', 3204),\n",
       "                           ('n_true_entities', 3174)])),\n",
       "             ('__total__',\n",
       "              OrderedDict([('n_predicted_entities', 3204),\n",
       "                           ('n_true_entities', 3174),\n",
       "                           ('precision', 83.70786516853933),\n",
       "                           ('recall', 84.49905482041588),\n",
       "                           ('f1', 84.10159924741298)]))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net3.fit(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3.save('../nm/ner/cnn_highway_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning\n",
    "model_params = {\"filter_width\": np.arange(3, 7), # if cnn\n",
    "                \"embeddings_dropout\": [True, False],\n",
    "                \"use_batch_norm\": [True, False], # if cnn\n",
    "                \"use_crf\": [True, False],\n",
    "                \"net_type\": ['cnn', 'rnn', 'cnn_highway'],\n",
    "                \"use_capitalization\": [True, False],\n",
    "                \"cell_type\": ['lstm', 'gru'] # if rnn\n",
    "               }\n",
    "training_params = {'dropout_rate': np.arange(0.1, 0.91, 0.2),\n",
    "                   'epochs': np.arange(10, 101, 30),\n",
    "                   'learning_rate':np.linspace(0.0001, 0.001, 10),\n",
    "                   'batch_size': np.arange(4, 65, 20),\n",
    "                   'learning_rate_decay': np.arange(0.5, 1.1, 0.1)}\n",
    "\n",
    "GridSearchCV()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
